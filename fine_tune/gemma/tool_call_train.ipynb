{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers torch trl accelerate peft datasets bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_call_format(system, tools, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                {tools}\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text\n",
    "\n",
    "def general_format(system, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tool_call_system = \"You are a helpful AI assistant that has to a set of tools listed between the <tools> xml tags that you may call to help the user. Only use them if the user query requires them. For each tool call return a json object with the name of the tool and its arguments surrounded by <tool_call> xml tags. If you decide not to use a tool respond normally and answer the user's query.\"\n",
    "general_system = \"You are a helpful AI assistant and you answer the user's questions clearly and correctly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    conversation = examples[\"conversations\"]\n",
    "    tools = examples[\"tools\"]\n",
    "\n",
    "    user = conversation[0][\"value\"]\n",
    "    response = conversation[1][\"value\"]\n",
    "    response_type = conversation[1][\"from\"]\n",
    "\n",
    "    response = response if response_type == \"gpt\" else f\"<tool_call>\\n{response}\\n</tool_call>\"\n",
    "\n",
    "    if tools.strip() in (\"[]\", \"\"):\n",
    "        text = general_format(general_system, user, response) + EOS_TOKEN\n",
    "    else:\n",
    "        tools = f\"<tools>\\n{tools}\\n</tools>\"\n",
    "        text = tool_call_format(tool_call_system, tools, user, response) + EOS_TOKEN\n",
    "        \n",
    "    return { \"text\" : text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"llamafactory/glaive_toolcall_en\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "  output_dir = \"main\",\n",
    "  max_steps=500,\n",
    "  per_device_train_batch_size = 1,\n",
    "  weight_decay = 1e-3,\n",
    "  warmup_steps = 50,\n",
    "  logging_steps = 10,\n",
    "  logging_dir=\"logs\",\n",
    "  save_strategy = \"steps\",\n",
    "  eval_strategy= \"steps\",\n",
    "  eval_steps = 100,\n",
    "  save_steps = 100,\n",
    "  learning_rate = 1e-4,\n",
    "  bf16 = True,\n",
    "  lr_scheduler_type = 'cosine',\n",
    "  seed = 3407, \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=peft_model,\n",
    "  max_seq_length = 4096,\n",
    "  dataset_text_field = \"text\",\n",
    "  tokenizer=tokenizer,\n",
    "  packing=False,\n",
    "  args=args,\n",
    "  train_dataset=dataset['train'],\n",
    "  eval_dataset=dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_prompt = \"\"\"<start_of_turn>user\n",
    "{}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "tool_list = \"\"\"\\n<tools>\n",
    "[{\n",
    "  \"name\": \"move_file\",\n",
    "  \"description\": \"Move a file from one location to another\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"source\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The file that should be moved. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "      \"target\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The destination of where the file should be moved to. It doesnt have to be a worded explantion.\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"source\", \"target\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"copy_file\",\n",
    "  \"description\": \"Copy a file from one location to another\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"source\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The file that should be copied. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "      \"target\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The destination of where the file should be copied to. It doesnt have to be a worded explantion.\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"source\", \"target\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"rename_file\",\n",
    "  \"description\": \"Rename a file.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"source\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The file that should be rename. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "      \"new_name\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The exact name of the new file.\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"source\", \"new_name\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"goto_file\",\n",
    "  \"description\": \"Go to a file.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"target\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The target file. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "    \"required\": [\"target\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"open_file\",\n",
    "  \"description\": \"Open any type of file on the computer.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"target\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The target file. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "    \"required\": [\"target\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"delete_file\",\n",
    "  \"description\": \"Delete a file.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"target\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The target file. It doesnt have to be a path just a worded explantion.\"\n",
    "      },\n",
    "    \"required\": [\"target\"]\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"name\": \"local_search\",\n",
    "  \"description\": \"Retrieve relevant information from local knowledge base to enhance response accuracy for the user's specific query.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"query\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The user's query. The thing they are looking for.\"\n",
    "      },\n",
    "    \"required\": [\"query\"]\n",
    "  }\n",
    "}]\n",
    "</tools>\"\"\"\n",
    "\n",
    "prompt = \"\\nOpen the apple image.\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    gemma_prompt.format(\n",
    "        tool_call_system + tool_list + prompt, \n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = peft_model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"lora_model\") \n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_name = \"trishonc/gemma-2-2b-it-tool-use\"\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"./merged_model\",\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
