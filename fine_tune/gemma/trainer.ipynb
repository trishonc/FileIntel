{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch\n",
    "!pip install transformers trl accelerate peft datasets bitsandbytes huggingface_hub\n",
    "!pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 256,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = True, \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_call_format(system, tools, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                <tools>\n",
    "                {tools}\n",
    "                </tools>\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                <tool_call>\n",
    "                {response}\n",
    "                </tool_call>\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def rag_format(system, context, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                <context>\n",
    "                {context}\n",
    "                </context>\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def general_format(system, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>   \n",
    "            \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "tool_call_system = \"You are a helpful AI assistant that has to a set of tools listed between the <tools> xml tags that you may use to help the user. Only use them if the user query requires them. For each tool call return a json object with the name of the tool and its arguments surrounded by <tool_call> xml tags\"\n",
    "rag_system = \"You are a helpful AI assistant and you should answer the user's query based on the provided context. Try to answer briefly and clearly. Make sure to derive your answer from the context as much as possible.\"\n",
    "general_system = \"You are a helpful AI assistant and you answer the user's questions accurately and clearly. For more complex questions you may write down your reasoning steps to avoid mistakes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    user = examples[\"user\"]\n",
    "    tools = examples[\"tools\"]\n",
    "    context = examples[\"context\"]\n",
    "    response = examples[\"response\"]\n",
    "    \n",
    "    if tools:\n",
    "        text = tool_call_format(tool_call_system, tools, user, response) + EOS_TOKEN\n",
    "    elif context:\n",
    "        text = rag_format(rag_system, context, user, response) + EOS_TOKEN\n",
    "    else:\n",
    "        text = general_format(general_system, user, response) + EOS_TOKEN\n",
    "       \n",
    "    return { \"text\" : text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trishonc/agent-buff\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "  output_dir = \"main\",\n",
    "  num_train_epochs = 1,\n",
    "  per_device_train_batch_size = 4,\n",
    "  gradient_accumulation_steps = 4,\n",
    "  weight_decay = 1e-3,\n",
    "  warmup_steps = 25,\n",
    "  logging_steps = 10,\n",
    "  logging_dir=\"logs\",\n",
    "  save_strategy = \"steps\",\n",
    "  evaluation_strategy= \"steps\",\n",
    "  eval_steps = 85,\n",
    "  save_steps = 85,\n",
    "  learning_rate = 1e-4,\n",
    "  fp16 = not is_bfloat16_supported,\n",
    "  bf16 = is_bfloat16_supported,\n",
    "  lr_scheduler_type = 'cosine',\n",
    "  seed = 3407, \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=peft_model,\n",
    "  max_seq_length = 4096,\n",
    "  dataset_text_field = \"text\",\n",
    "  tokenizer=tokenizer,\n",
    "  packing=False,\n",
    "  args=args,\n",
    "  train_dataset=dataset['train'],\n",
    "  eval_dataset=dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Lee had $10 and his friend had $8. They went to a restaurant where they ordered chicken wings for $6 and a chicken salad for some amount. They also got 2 sodas for $1.00 each. The tax came to $3. They received $3 in change in total. How much did the chicken salad cost?\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([general_format(general_system, question, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"lora_model\") \n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "peft_model.push_to_hub_merged(\"trishonc/gemma-2-2b-it-buffed\", tokenizer, save_method = \"merged_16bit\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
