{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers torch trl accelerate peft datasets bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_call_format(system, tools, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                <tools>\n",
    "                {tools}\n",
    "                </tools>\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                <tool_call>\n",
    "                {response}\n",
    "                </tool_call>\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def rag_format(system, context, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                <context>\n",
    "                {context}\n",
    "                </context>\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def general_format(system, user, response):\n",
    "    text = f\"\"\"<start_of_turn>user\n",
    "                {system}\n",
    "                {user}\n",
    "                <end_of_turn>\n",
    "                <start_of_turn>model\n",
    "                {response}\n",
    "                <end_of_turn>\n",
    "            \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "tool_call_system = \"You are a helpful AI assistant that has to a set of tools listed between the <tools> xml tags that you may use to help the user. Only use them if the user query requires them. For each tool call return a json object with the name of the tool and its arguments surrounded by <tool_call> xml tags\"\n",
    "rag_system = \"You are a helpful AI assistant and you should answer the user's query based on the provided context. Try to answer briefly and clearly. Make sure to derive your answer from the context as much as possible.\"\n",
    "general_system = \"You are a helpful AI assistant and you answer the user's questions accurately and clearly. For more complex questions you may write down your reasoning steps to avoid mistakes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    user = examples[\"user\"]\n",
    "    tools = examples[\"tools\"]\n",
    "    context = examples[\"context\"]\n",
    "    response = examples[\"response\"]\n",
    "    \n",
    "    if tools:\n",
    "        text = tool_call_format(tool_call_system, tools, user, response) + EOS_TOKEN\n",
    "    elif context:\n",
    "        text = rag_format(rag_system, context, user, response) + EOS_TOKEN\n",
    "    else:\n",
    "        text = general_format(general_system, user, response) + EOS_TOKEN\n",
    "       \n",
    "    return { \"text\" : text }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"trishonc/agent-buff\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "  output_dir = \"main\",\n",
    "  num_train_epochs = 1,\n",
    "  per_device_train_batch_size = 1,\n",
    "  weight_decay = 1e-3,\n",
    "  warmup_steps = 50,\n",
    "  logging_steps = 10,\n",
    "  logging_dir=\"logs\",\n",
    "  save_strategy = \"steps\",\n",
    "  eval_strategy= \"steps\",\n",
    "  eval_steps = 300,\n",
    "  save_steps = 300,\n",
    "  learning_rate = 1e-4,\n",
    "  bf16 = True,\n",
    "  lr_scheduler_type = 'cosine',\n",
    "  seed = 3407, \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=peft_model,\n",
    "  max_seq_length = 4096,\n",
    "  dataset_text_field = \"text\",\n",
    "  tokenizer=tokenizer,\n",
    "  packing=False,\n",
    "  args=args,\n",
    "  train_dataset=dataset['train'],\n",
    "  eval_dataset=dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_prompt = \"\"\"<start_of_turn>user\n",
    "{}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Fifteen sheets of colored tape, each 25 centimeters (cm) long, were joined together with an overlap of 0.5 centimeters (cm). How many meters (m) is the total length of the continuous color tape?\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    gemma_prompt.format(\n",
    "        general_system + prompt, \n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = peft_model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"lora_model\") \n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "repo_name = \"trishonc/gemma-2-2b-it-buffed\"\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"./merged_model\",\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
